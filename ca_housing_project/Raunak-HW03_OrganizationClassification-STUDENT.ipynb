{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03: Loss Functions and Classification\n",
    "\n",
    "### <p style=\"text-align: right;\"> &#9989; Put your name here\n",
    "\n",
    "<!-- ![image.png](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png) -->\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" \n",
    "     alt=\"Palmer Penguins\" \n",
    "     style=\"width: 600px; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ **Learning Goals**\n",
    "\n",
    "1. Learn to organize a machine learning project using a professional directory structure and clean workflow practices.\n",
    "\n",
    "2. Understand binary classification with logistic regression, including loss functions, decision boundaries, and model interpretation.\n",
    "\n",
    "3. Apply grid search to optimize logistic regression parameters and visualize decision boundaries and probability distributions.\n",
    "\n",
    "4. Explore multi-class classification using one-vs-rest logistic regression with the Palmer Penguins dataset, and compare it to direct multi-class approaches.\n",
    "\n",
    "**This assignment is due by 11:59 p.m. Friday September 19,** and should be uploaded into the appropriate \"Homework\" submission folder on D2L.  Submission instructions can be found at the end of the notebook.\n",
    "\n",
    "\n",
    "**üìö Preparation:** Read Chapter 3 of the textbook and complete the following assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Project Organization and Workflow (20 points)\n",
    "\n",
    "The goal of this problem is to organize your machine learning project in a structured, professional manner. **Project organization is absolutely critical** in data science work as it enables reproducibility, facilitates collaboration with team members, maintains clean and efficient workflows, and most importantly, **allows for easy sharing and handoff of your work** to colleagues, stakeholders, or the broader community. A well-organized project can be understood and utilized by others without extensive explanation, making your work truly impactful and professional.\n",
    "\n",
    "**Important Note:** There is no single \"correct\" way to organize data science projects - folder structures and organization patterns often depend on the specific task, team preferences, project complexity, and organizational standards. However, consistency and logical structure are always essential. For this assignment, we have chosen a particular organizational structure that represents common industry practices and will help you develop good organizational habits that you can adapt to different contexts throughout your career.\n",
    "\n",
    "#### Required Directory Structure\n",
    "\n",
    "You must organize your GitHub repository according to the following structure:\n",
    "\n",
    "```\n",
    "/ca_housing_project\n",
    "‚îú‚îÄ‚îÄ /data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ /raw\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ /train  \n",
    "‚îÇ   ‚îî‚îÄ‚îÄ /test\n",
    "‚îú‚îÄ‚îÄ /images\n",
    "‚îú‚îÄ‚îÄ /models\n",
    "‚îî‚îÄ‚îÄ /analysis\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "#### Task Instructions\n",
    "\n",
    "**üìö Preparation:** Complete Parts 1-3 and have the Chapter 2 notebook code available from the textbook.\n",
    "\n",
    "**üóíÔ∏è Task: (37 points)** Organize your project files according to the specifications below. You will be graded on proper file placement, code organization, and adherence to the directory structure.\n",
    "\n",
    "#### `/data/raw`\n",
    "- Store the **original, unmodified housing dataset**, *i.e.* the CSV file `housing.csv`\n",
    "- This file should never be modified or processed\n",
    "\n",
    "#### `/data/test`\n",
    "- Store the **raw testing set**, containing the features and target variable created from stratified splitting\n",
    "- This should contain **only 13 columns**\n",
    "- File should be named descriptively (e.g., `housing_test.csv`)\n",
    "\n",
    "#### `/data/train` \n",
    "- Store **two versions** of the training data:\n",
    "  1. **Raw training set**: Stratified split with **13 columns only**\n",
    "  2. **Processed training set**: After EDA with **24 features** (including engineered features)\n",
    "- Use clear, descriptive filenames (e.g., `housing_train.csv`, `housing_train_processed.csv`)\n",
    "\n",
    "#### `/images` Directory\n",
    "This directory should contain all the images and plots created in the project. This directory is automatically created in the notebook from the textbook. \n",
    "\n",
    "#### `/analysis` Directory \n",
    "It should contain the following files.\n",
    "\n",
    "#### `ida.ipynb` - Initial Data Analysis Notebook\n",
    "- Contains code for **Initial Data Analysis**, *e.g.* up to dataset splitting\n",
    "- Should include sections, clearly identified via markdown cells, for:\n",
    "  - Data loading\n",
    "  - Data type analysis  \n",
    "  - Stratified train/test splitting\n",
    "- **Final section**: Code for saving training and testing sets to appropriate folders\n",
    "- Use clear markdown headers to organize sections\n",
    "\n",
    "#### `eda.ipynb` - Exploratory Data Analysis Notebook  \n",
    "- Contains code from textbook for conducting **Exploratory Data Analysis**\n",
    "- Should include sections for:\n",
    "  - Geographic data visualization\n",
    "  - Feature correlation analysis\n",
    "  - Feature engineering and creation\n",
    "- **Final section**: Code for saving the processed dataset (24 features) to the training folder\n",
    "- Use clear markdown headers to organize sections\n",
    "\n",
    "#### `preprocessing_pipeline.py` - Python Script\n",
    "- A **Python script** (not notebook) that:\n",
    "  - Reads the raw training set\n",
    "  - Uses a **scikit-learn Pipeline** to process the dataset\n",
    "  - Saves the final processed dataset to the training folder\n",
    "- Should be executable from command line\n",
    "- Include proper imports and comments\n",
    "\n",
    "#### `/models` Directory\n",
    "\n",
    "The `models` directory should contain **separate notebooks** for each model type:\n",
    "- `LinearRegression.ipynb`\n",
    "- `DecisionTree.ipynb` \n",
    "- `RandomForest.ipynb`\n",
    "- `SVR.ipynb`\n",
    "\n",
    "#### `README.md` file\n",
    "\n",
    "A README file explaining the content of the repo. \n",
    "\n",
    "\n",
    "Each model notebook must contain the following **clearly marked sections** using Markdown headers:\n",
    "\n",
    "1. **# Data Loading**\n",
    "   - Read the cleaned/processed dataset (24 features)\n",
    "   - No pipelines needed since data is already preprocessed\n",
    "\n",
    "2. **# Model Fitting** \n",
    "   - Initialize and fit the specific model\n",
    "   - Display basic training results\n",
    "\n",
    "3. **# Cross-Validation**\n",
    "   - Implement cross-validation evaluation\n",
    "   - Display CV scores and statistics\n",
    "\n",
    "4. **# Hyperparameter Tuning**\n",
    "   - Use GridSearchCV or RandomizedSearchCV\n",
    "   - Show best parameters and improved performance\n",
    "\n",
    "5. **# Model Saving**\n",
    "   - Save the trained model to the `/models` directory\n",
    "   - Use appropriate naming convention (e.g., `linear_regression_model.pkl`)\n",
    "\n",
    "\n",
    "#### Submission Guidelines\n",
    "\n",
    "**File Organization Requirements:**\n",
    "- All files must be in their correct directories\n",
    "- Use descriptive, consistent naming conventions\n",
    "- No unnecessary or duplicate files\n",
    "\n",
    "**Code Quality Requirements:**\n",
    "- All notebooks should run without errors\n",
    "- Use clear markdown headers for section organization\n",
    "- Include appropriate comments in code\n",
    "- Python script must be executable and functional\n",
    "\n",
    "**Documentation Requirements:**\n",
    "- Each notebook should have a brief introduction explaining its purpose\n",
    "- Code cells should be well-documented\n",
    "- File paths should be relative to the project root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Binary Classification with Logistic Regression (20 points)\n",
    "\n",
    "Now let's move to classification with four points in 2D:\n",
    "- Class 0: (1,0), (2,0)\n",
    "- Class 1: (3,1), (4,1)\n",
    "\n",
    "Think about the encoding: what values will your $y_i$ take? \n",
    "\n",
    "Let's write $x_1$ for the x-coordinate and $x_2$ for the $y$-coordinate. Our model computes the probability:\n",
    "$$P(y=1|\\mathbf{x}) = \\sigma(w_0 + w_1x_1 + w_2x_2)$$\n",
    "\n",
    "where $\\sigma(z)$ is the sigmoid function:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}.$$\n",
    "\n",
    "We will use the loss function (binary cross-entropy):\n",
    "$$L(w_0,w_1,w_2) = -\\frac{1}{4}\\sum_{i=1}^4 [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)]$$\n",
    "where $p_i = \\sigma(w_0 + w_1x_{1i} + w_2x_{2i})$.\n",
    "\n",
    "üóíÔ∏è **Task 2.1: (4 points)** Plot the four points in the plane, using different colors for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Task 2.2 (5 points):**  \n",
    "   - Write out the gradients with respect to $w_0$, $w_1$, and $w_2$\n",
    "   - Set them to zero and comment on what you find\n",
    "   - You don't need to solve these equations, but explain why they're challenging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text answer"
    ]
   },
   "source": [
    "‚úèÔ∏è **Answer:** \n",
    "*Put your answers here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Task 2.3 (5 points):** The decision boundary occurs where $P(y=1|x) = 0.5$, which is where:\n",
    "   $w_0 + w_1x_1 + w_2x_2 = 0$\n",
    "   - What shape is this in the $x_1-x_2$ plane?\n",
    "   - Add a possible decision boundary to your plot (that is, guess where it might be and plot your guess)\n",
    "   - How many different possible lines could separate these points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text answer"
    ]
   },
   "source": [
    "‚úèÔ∏è **Answer:** \n",
    "*Put your answers here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Task 2.4 (5 points):** If you have a new point at $(2.5, 0.5)$:\n",
    "   - Where might it fall relative to your proposed decision boundary? (add a marker X to your plot at this new point)\n",
    "   - What probability would you expect the model to assign to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text answer"
    ]
   },
   "source": [
    "‚úèÔ∏è **Answer:** \n",
    "*Put your answers here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Finding and Visualizing the Decision Boundary (20 points)\n",
    "\n",
    "#### The Model\n",
    "\n",
    "In logistic regression for 2D classification, we model the probability that $y=1$ given input coordinates $(x_1,x_2)$:\n",
    "$$P(y=1|\\mathbf{x}) = \\sigma(w_0 + w_1x_1 + w_2x_2)$$\n",
    "where œÉ is the sigmoid (logistic) function:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}.$$\n",
    "\n",
    "#### The Loss Function\n",
    "For binary classification with labels $y ‚àà \\{0,1\\}$, we use binary cross-entropy loss:\n",
    "$$\n",
    "L(w_0,w_1,w_2) = -\\frac{1}{n}\\sum_{i=1}^n [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)]\n",
    "$$\n",
    "where $p_i = P(y=1|x_i) = œÉ(w_0 + w_1x_{1i} + w_2x_{2i})$\n",
    "\n",
    "#### The Decision Boundary\n",
    "- The decision boundary occurs where $P(y=1|x) = 0.5$\n",
    "- Since $\\sigma(0) = 0.5$, this happens when $w_0 + w_1x_1 + w_2x_2 = 0$\n",
    "- This creates a line in the $x_1-x_2$ plane with:\n",
    "  - slope = $-w_1/w_2$\n",
    "  - intercept = $-w_0/w_2$\n",
    "\n",
    "üóíÔ∏è **Tasks: 3.1 (1 points)** Given four points:\n",
    "- Class 0: (1,0), (2,0)\n",
    "- Class 1: (3,1), (4,1)\n",
    "\n",
    "Find the \"best\" parameters for this model using **grid search** written in Python (no `sklearn`, `numpy` or other libraries!!). A helper code is given below if you wish to use it. (In reality we might use something more efficient, like a gradient descent algorithm, but I want this to be very robust, clear and focussed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER\n",
    "\n",
    "# helper code if you want to use it\n",
    "# if you use this, heavily comment it so that you can do it yourself next time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "X = np.array([[1,0], [2,0], [3,1], [4,1]])  # Each row is [x‚ÇÅ,x‚ÇÇ]\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "def compute_loss(w0, w1, w2, X, y):\n",
    "    \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "    z = w0 + w1*X[:,0] + w2*X[:,1]  # X[:,0] is x‚ÇÅ, X[:,1] is x‚ÇÇ\n",
    "    p = 1/(1 + np.exp(-z))  # sigmoid\n",
    "    eps = 1e-15  # avoid log(0)\n",
    "    return -np.mean(y * np.log(p + eps) + (1-y) * np.log(1 - p + eps))\n",
    "\n",
    "# Grid search over parameters\n",
    "#   grid search is easy to understand, easy to code and ensures you cover the span of the parameters\n",
    "#   but, it can be slow and inefficient\n",
    "w0_range = np.linspace(-10, 10, 20)\n",
    "w1_range = np.linspace(-10, 10, 20)\n",
    "w2_range = np.linspace(-10, 10, 20)\n",
    "best_loss = float('inf')\n",
    "best_w0, best_w1, best_w2 = 0, 0, 0\n",
    "\n",
    "# Brute-force grid search add your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Tasks: 3.2 (2 points)** Using your best parameters:\n",
    "   - Write the equation of your decision boundary line\n",
    "   - Calculate $P(y=1|x)$ for each training point\n",
    "   - What is the slope and intercept of your boundary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text answer"
    ]
   },
   "source": [
    "‚úèÔ∏è **Answer:** \n",
    "*Put your answers here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Tasks: 3.3 (1 points)** Visualize the probability of being in class 1 and add the decision boundary to your plot. There might be helper code below if you are interested. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code answer"
    ]
   },
   "outputs": [],
   "source": [
    "### ANSWER\n",
    "# \n",
    "# helper code if you want to use it\n",
    "# if you use this, heavily comment it so that you can do it yourself next time\n",
    "\n",
    "# Create a grid of points\n",
    "x1, x2 = np.meshgrid(np.linspace(0, 5, 100),\n",
    "                     np.linspace(-0.5, 1.5, 100))\n",
    "# Compute probabilities for each point\n",
    "Z = 1/(1 + np.exp(-(best_w0 + best_w1*x1 + best_w2*x2)))\n",
    "\n",
    "# Plot probability heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(x1, x2, Z, levels=20, cmap='RdBu', alpha=0.7)\n",
    "plt.colorbar(label='P(y=1|x)')\n",
    "\n",
    "# Plot decision boundary (where P=0.5)\n",
    "x1_bd = np.linspace(0, 5, 100)\n",
    "x2_bd = -(best_w0 + best_w1*x1_bd)/best_w2\n",
    "\n",
    "# Modify the code below to show the decision boundary\n",
    "plt.contour(x1, x2, Z, levels=[???], colors='k', linestyles='--', label='Decision Boundary')\n",
    "\n",
    "\n",
    "# Plot original points\n",
    "plt.scatter(X[y==0,0], X[y==0,1], color='blue', label='Class 0')\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color='red', label='Class 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Tasks: 3.4 (16 points)** Interpretation (give detailed answers in a markdown cell):\n",
    "   - How does the probability change as you move perpendicular to the decision boundary?\n",
    "   - What determines how quickly probabilities change near the boundary?\n",
    "   - How confident is your model in regions far from the training data?\n",
    "   - For what applications might you want probabilities rather than just classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text answer"
    ]
   },
   "source": [
    "‚úèÔ∏è **Answer:** \n",
    "*Put your answers here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 4: Multi-class Classification with Penguins (20 points)\n",
    "\n",
    "We'll use the Palmer Penguins dataset to explore multi-class classification using one-vs-rest (OvR) with SGD.\n",
    "\n",
    "The one-vs-rest approach trains three separate binary classifiers:\n",
    "- Adelie vs rest: $P(y=0|x)$ vs $P(y\\neq 0|x)$\n",
    "- Gentoo vs rest: $P(y=1|x)$ vs $P(y\\neq 1|x)$\n",
    "- Chinstrap vs rest: $P(y=2|x)$ vs $P(y\\neq 2|x)$\n",
    "\n",
    "Each classifier uses logistic regression with loss:\n",
    "$$L_k(w) = -\\frac{1}{n}\\sum_{i=1}^n [y_{ki}\\log(p_{ki}) + (1-y_{ki})\\log(1-p_{ki})]$$\n",
    "where $y_{ki}$ is 1 if example $i$ is class $k$ and 0 otherwise.\n",
    "\n",
    "Some helper code is given below to get this problem set up quickly. \n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. (5 points) Visualization Setup: Complete the visualization code below and create two plots:\n",
    "- Original data points colored by species\n",
    "- Decision regions after fitting the classifier\n",
    "\n",
    "2. (5 points) Binary Boundaries\n",
    "The one-vs-rest approach creates three binary boundaries. For each classifier:\n",
    "- Plot the binary decision boundary (provide code)\n",
    "- Identify regions where boundaries disagree\n",
    "- What happens in these regions?\n",
    "\n",
    "3. (5 points) Class Probabilities\n",
    "Choose a point near a decision boundary and:\n",
    "- Get probabilities for each class using `clf.predict_proba()`\n",
    "- Explain how these relate to the three binary classifiers\n",
    "- Does the highest probability always win?\n",
    "\n",
    "4. (5 points) Reflection\n",
    "Compare one-vs-rest to direct multi-class logistic regression:\n",
    "- What are the advantages/disadvantages of each?\n",
    "- When might you prefer one over the other?\n",
    "- How do the decision boundaries differ?\n",
    "\n",
    "Hints:\n",
    "- Use `clf.fit(X_scaled, y)` to train the model\n",
    "- Access binary classifiers with `clf.estimators_`\n",
    "- For binary boundaries, create separate classifiers with:\n",
    "  ```python\n",
    "  y_binary = (y == class_label).astype(int)\n",
    "  clf_binary = SGDClassifier(loss='log_loss')\n",
    "  clf_binary.fit(X_scaled, y_binary)\n",
    "  ```\n",
    "- Check species encoding with `le.classes_` to confirm labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load data\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Prepare features (bill length and depth) and target (species)\n",
    "# I am using .dropna() here for simplicity, but I don't generally recommend it!\n",
    "X = penguins[['bill_length_mm', 'bill_depth_mm']].dropna()\n",
    "y = penguins['species'].dropna()\n",
    "\n",
    "# Convert species to numeric labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # 0:Adelie, 1:Gentoo, 2:Chinstrap\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create OvR classifier\n",
    "clf = SGDClassifier(loss='log_loss', max_iter=1000, random_state=42)\n",
    "\n",
    "# what is next?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, clf, title):\n",
    "    # Create meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                        np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Get predictions\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Standardized Bill Length')\n",
    "    plt.ylabel('Standardized Bill Depth')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2025, Department of Computational Mathematics, Science and Engineering at Michigan State University."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmse492",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
